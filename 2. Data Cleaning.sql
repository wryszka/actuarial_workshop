{"version":"NotebookV1","origId":455928964370642,"name":"2. Data Cleaning","language":"sql","commands":[{"version":"CommandV1","origId":455928964370643,"guid":"24d8adaf-b25a-4cf8-a3d3-4bec367a7aef","subtype":"command","commandType":"auto","position":1.0,"command":"%md\nWe get data from another team. It's saved in claims_raw table. We often experience issues with the data. Let's do some basic checks!","commandVersion":10,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"f40b25ce-a2ee-4cff-96c0-766794f5cbee"},{"version":"CommandV1","origId":455928964370686,"guid":"f96ad606-a201-4fce-a76d-1d284e949b69","subtype":"command","commandType":"auto","position":2.0,"command":"SELECT policy_id, accident_date\nFROM claims_raw\nWHERE accident_date < '2020-01-01';","commandVersion":5,"state":"finished","results":{"type":"listResults","data":[{"type":"table","data":[[770,"2019-01-31T00:00:00.000Z"],[959,"2019-06-27T00:00:00.000Z"],[377,"2019-09-04T00:00:00.000Z"],[551,"2019-06-20T00:00:00.000Z"],[605,"2019-03-11T00:00:00.000Z"],[387,"2019-01-27T00:00:00.000Z"],[975,"2019-05-30T00:00:00.000Z"],[357,"2019-10-07T00:00:00.000Z"],[921,"2019-08-02T00:00:00.000Z"],[408,"2019-11-19T00:00:00.000Z"],[611,"2019-06-07T00:00:00.000Z"],[634,"2019-12-06T00:00:00.000Z"],[30,"2019-07-06T00:00:00.000Z"],[800,"2019-02-14T00:00:00.000Z"],[195,"2019-01-16T00:00:00.000Z"],[847,"2019-01-31T00:00:00.000Z"],[568,"2019-11-21T00:00:00.000Z"],[787,"2019-09-02T00:00:00.000Z"],[202,"2019-07-22T00:00:00.000Z"],[988,"2019-05-28T00:00:00.000Z"],[479,"2019-10-31T00:00:00.000Z"],[817,"2019-04-27T00:00:00.000Z"],[628,"2019-12-30T00:00:00.000Z"],[434,"2019-01-20T00:00:00.000Z"],[75,"2019-10-06T00:00:00.000Z"],[448,"2019-07-22T00:00:00.000Z"],[827,"2019-01-19T00:00:00.000Z"],[245,"2019-09-30T00:00:00.000Z"],[485,"2019-05-22T00:00:00.000Z"],[869,"2019-07-17T00:00:00.000Z"],[201,"2019-01-05T00:00:00.000Z"],[947,"2019-03-30T00:00:00.000Z"],[497,"2019-04-17T00:00:00.000Z"],[385,"2019-06-30T00:00:00.000Z"],[959,"2019-02-14T00:00:00.000Z"],[280,"2019-08-30T00:00:00.000Z"],[111,"2019-10-10T00:00:00.000Z"],[764,"2019-04-18T00:00:00.000Z"],[762,"2019-09-28T00:00:00.000Z"]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"policy_id","type":"\"long\"","metadata":"{}"},{"name":"accident_date","type":"\"timestamp\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.connect.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"policy_id","nullable":true,"type":"long"},{"metadata":{},"name":"accident_date","nullable":true,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null}],"columnCustomDisplayInfos":{},"metadata":{"dataframeName":"_sqldf","executionCount":3,"createTempViewForImplicitDf":true}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.connect.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"policy_id","nullable":true,"type":"long"},{"metadata":{},"name":"accident_date","nullable":true,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1731596731891,"submitTime":1731596724779,"finishTime":1731596735534,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["table",39]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"cc624fd1-eb53-4b44-95ec-218bdfca4199"},{"version":"CommandV1","origId":455928964370699,"guid":"23098252-6d63-47a5-b872-f914a0a5f18c","subtype":"command","commandType":"auto","position":3.0,"command":"%md\nSome issues!\nLet's now check the claim amount. We don't expect to see more than 100k payout.\n","commandVersion":8,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"bfcfd5e2-5aca-4617-aa63-54f4c61c281c"},{"version":"CommandV1","origId":455928964370724,"guid":"bced1006-e92a-4598-816e-4aa70f826e4f","subtype":"command","commandType":"auto","position":1.5,"command":"select count(*) from claims_raw","commandVersion":7,"state":"finished","results":{"type":"listResults","data":[{"type":"table","data":[[235]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"count(*)","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.connect.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"count(*)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"columnCustomDisplayInfos":{},"metadata":{"dataframeName":"_sqldf","executionCount":2,"createTempViewForImplicitDf":true}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.connect.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"count(*)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1731596717914,"submitTime":1731596699362,"finishTime":1731596731723,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["table",1]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"b0f74848-b3b4-4b7b-bb9b-b1223b5f2596"},{"version":"CommandV1","origId":455928964370762,"guid":"f082df9b-aead-4b36-87f9-064908030e9f","subtype":"command","commandType":"auto","position":1.75,"command":"%md\nWe expect all our data to be dated from 2020 onwards. Let's see if there are any accident dates before 2020.\n","commandVersion":8,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"729c7402-9a72-4420-8b5f-ab80f07e7508"},{"version":"CommandV1","origId":455928964370763,"guid":"e3de58fd-ccc7-4345-9667-9c28e8c1050f","subtype":"command","commandType":"auto","position":4.0,"command":"SELECT policy_id, cumulative_paid FROM claims_raw\nWHERE cumulative_paid > 100000;","commandVersion":9,"state":"finished","results":{"type":"listResults","data":[{"type":"table","data":[[49,50000000]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"policy_id","type":"\"long\"","metadata":"{}"},{"name":"cumulative_paid","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.connect.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"policy_id","nullable":true,"type":"long"},{"metadata":{},"name":"cumulative_paid","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"columnCustomDisplayInfos":{},"metadata":{"dataframeName":"_sqldf","executionCount":6,"createTempViewForImplicitDf":true}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.connect.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"policy_id","nullable":true,"type":"long"},{"metadata":{},"name":"cumulative_paid","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1731596825568,"submitTime":1731596825031,"finishTime":1731596828428,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["table",1]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"cf23cc37-2320-48ad-ac38-68ead1f2cca5"},{"version":"CommandV1","origId":455928964370820,"guid":"450c42db-ce09-4dfd-940b-ae29593d831d","subtype":"command","commandType":"auto","position":5.0,"command":"%md\nOK, it seems our data has issues. Please move to '2a. Data Cleansing Pipeline' to resolve this.\n","commandVersion":8,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"dc016ac9-057c-4651-b795-7b4e0a8a7dac"},{"version":"CommandV1","origId":455928964372324,"guid":"9c1d20fe-abcd-4ff2-97bc-85b4b9c1fa33","subtype":"command","commandType":"auto","position":0.5,"command":"%python\n# Execute SQL command and get the current user\nuser = spark.sql(\"SELECT current_user()\").collect()[0][0]\n\n# Replace characters in the user string to create the catalog name\ncatalog_name = user.replace(\".\", \"_\").replace(\"@\", \"_\")\n\n# Display the catalog name\ndisplay(catalog_name)\n\n# Use the formatted catalog name in the USE CATALOG SQL command\nspark.sql(f\"USE CATALOG {catalog_name}\")\n\n# Set the schema to use\nspark.sql(\"USE SCHEMA actuarial_demo\")","commandVersion":8,"state":"finished","results":{"type":"listResults","data":[{"type":"mimeBundle","data":{"text/plain":"'laurence_ryszka_databricks_com'"},"executionCount":null,"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"kernelSessionId":"45e8da0d-ab8be59b21c5d279e6ede352"}},{"type":"mimeBundle","data":{"text/plain":"DataFrame[]"},"executionCount":1,"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"kernelSessionId":"45e8da0d-ab8be59b21c5d279e6ede352"}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":{"type":"baseError","stackFrames":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)","File \u001B[0;32m<command-455928964372324>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msql\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m# Execute SQL command and get the current user\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43muser = spark.sql(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSELECT current_user()\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m).collect()[0][0]\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m# Display the user\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mdisplay(user)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m# Replace characters in the user string to create the catalog name\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mcatalog_name = user.replace(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m).replace(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m@\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m# Display the catalog name\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mdisplay(catalog_name)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m# Use the formatted catalog name in the USE CATALOG SQL command\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mspark.sql(f\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUSE CATALOG \u001B[39;49m\u001B[38;5;132;43;01m{catalog_name}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m# Set the schema to use\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mspark.sql(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUSE SCHEMA actuarial_demo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2478\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2476\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2477\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2478\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2480\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2481\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2482\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n","File \u001B[0;32m/databricks/python_shell/dbruntime/sql_magic/sql_magic.py:199\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mlogDriverEvent({\n\u001B[1;32m    194\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventType\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark-connect-sql-end\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    195\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventId\u001B[39m\u001B[38;5;124m\"\u001B[39m: comm\u001B[38;5;241m.\u001B[39mcomm_id,  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseException\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    197\u001B[0m     })\n\u001B[1;32m    198\u001B[0m     comm\u001B[38;5;241m.\u001B[39msend({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: traceback\u001B[38;5;241m.\u001B[39mformat_exc()})\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    201\u001B[0m     remove_control_channel_comm_handler(comm\u001B[38;5;241m.\u001B[39mcomm_id)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n","File \u001B[0;32m/databricks/python_shell/dbruntime/sql_magic/sql_magic.py:149\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    147\u001B[0m max_num_choices \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxNumberOfChoices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_udf(request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mescaped_widget_values\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 149\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_query_request_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquery\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n","File \u001B[0;32m/databricks/python_shell/dbruntime/sql_magic/sql_magic.py:106\u001B[0m, in \u001B[0;36mSqlMagic.get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    105\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 106\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masserting_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidget_bindings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    108\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query)\n","File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/session.py:643\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args)\u001B[0m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msql\u001B[39m(\u001B[38;5;28mself\u001B[39m, sqlQuery: \u001B[38;5;28mstr\u001B[39m, args: Optional[Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], List]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    642\u001B[0m     cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, args)\n\u001B[0;32m--> 643\u001B[0m     data, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcmd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommand\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    644\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    645\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n","File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1203\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1201\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1202\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1203\u001B[0m data, _, _, _, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_and_fetch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_request_metadata\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1207\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (data\u001B[38;5;241m.\u001B[39mto_pandas(), properties)\n","File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1624\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1621\u001B[0m schema: Optional[StructType] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1622\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1624\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1625\u001B[0m     req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m []\n\u001B[1;32m   1626\u001B[0m ):\n\u001B[1;32m   1627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1628\u001B[0m         schema \u001B[38;5;241m=\u001B[39m response\n","File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1601\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1599\u001B[0m                     \u001B[38;5;28;01myield from\u001B[39;00m handle_response(b)\n\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1601\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1910\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   1908\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 1910\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_rpc_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1911\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   1912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n","File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1985\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   1982\u001B[0m             info \u001B[38;5;241m=\u001B[39m error_details_pb2\u001B[38;5;241m.\u001B[39mErrorInfo()\n\u001B[1;32m   1983\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[0;32m-> 1985\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   1986\u001B[0m                 info,\n\u001B[1;32m   1987\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   1988\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   1989\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   1990\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m   1992\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m   1993\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '#'. SQLSTATE: 42601 (line 1, pos 0)\n\n== SQL ==\n# Execute SQL command and get the current user\n^^^\nuser = spark.sql(\"SELECT current_user()\").collect()[0][0]\n\n# Display the user\ndisplay(user)\n\n# Replace characters in the user string to create the catalog name\ncatalog_name = user.replace(\".\", \"_\").replace(\"@\", \"_\")\n\n# Display the catalog name\ndisplay(catalog_name)\n\n# Use the formatted catalog name in the USE CATALOG SQL command\nspark.sql(f\"USE CATALOG {catalog_name}\")\n\n# Set the schema to use\nspark.sql(\"USE SCHEMA actuarial_demo\")\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:308)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:114)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:137)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)\n\tat com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:949)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:453)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:948)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2923)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2860)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:345)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:259)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:192)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:239)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:192)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:571)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:571)"],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{},"jupyterProps":{"ename":"ParseException","evalue":"\n[PARSE_SYNTAX_ERROR] Syntax error at or near '#'. SQLSTATE: 42601 (line 1, pos 0)\n\n== SQL ==\n# Execute SQL command and get the current user\n^^^\nuser = spark.sql(\"SELECT current_user()\").collect()[0][0]\n\n# Display the user\ndisplay(user)\n\n# Replace characters in the user string to create the catalog name\ncatalog_name = user.replace(\".\", \"_\").replace(\"@\", \"_\")\n\n# Display the catalog name\ndisplay(catalog_name)\n\n# Use the formatted catalog name in the USE CATALOG SQL command\nspark.sql(f\"USE CATALOG {catalog_name}\")\n\n# Set the schema to use\nspark.sql(\"USE SCHEMA actuarial_demo\")\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:308)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:114)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:137)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)\n\tat com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:949)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:453)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:948)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2923)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2860)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:345)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:259)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:192)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:239)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:192)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:571)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:571)"},"sqlProps":{"sqlState":"42601","errorClass":"PARSE_SYNTAX_ERROR","stackTrace":"org.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:308)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:114)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:137)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)\n\tat com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:949)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:453)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:948)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2923)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2860)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:345)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:259)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:192)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:239)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:192)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:571)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:571)","startIndex":null,"stopIndex":null,"pysparkCallSite":"","pysparkFragment":""}},"workflows":[],"startTime":1731596710413,"submitTime":1731596690439,"finishTime":1731596716328,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["mimeBundle",null],["mimeBundle",null]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"84cd1951-0539-4feb-aa3b-c9fc7be6c5d0"}],"dashboards":[],"guid":"7813cdf4-ba1c-4868-8032-a86fd553c9b8","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":2},"reposExportFormat":"SOURCE","environmentMetadata":{"client":"1","base_environment":""}}